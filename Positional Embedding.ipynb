{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc8a29c-1fbf-4963-8753-f2e429a0fa4d",
   "metadata": {},
   "source": [
    "## Positional Embedding ( Encoding Words Positions)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43bd3495-9030-40dc-88dd-88a954488a62",
   "metadata": {},
   "source": [
    "We now consider more realistic and useful embedding sizes and encode the input tokens into a 256-dimensional vector representation\n",
    "This is smaller than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f044bcdc-b845-42a3-8007-ca7b7a3470cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2025.9.18-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/envs/gensim_env/lib/python3.11/site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/gensim_env/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/gensim_env/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/gensim_env/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/gensim_env/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n",
      "Downloading tiktoken-0.12.0-cp311-cp311-macosx_10_12_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.18-cp311-cp311-macosx_10_9_x86_64.whl (288 kB)\n",
      "Installing collected packages: regex, tiktoken\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [tiktoken]1/2\u001b[0m [tiktoken]\n",
      "\u001b[1A\u001b[2KSuccessfully installed regex-2025.9.18 tiktoken-0.12.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f4e35a-cdd3-4e8c-b1ef-621d28791a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f65172-2764-4539-afba-98f71b9361d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/the-verdict.txt\",\"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59c591e5-992a-4c1c-8502-cf10ef6dd9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt,tokenizer,max_length,stride):\n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[]\n",
    "\n",
    "        #Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        #use a sliding window to chunk the book into overlapping sequence of max_length\n",
    "        # sliding window is take entire row or max_length of texts and then slide next row\n",
    "\n",
    "        for i in range(0, len(token_ids)- max_length, stride):\n",
    "            input_chunk= token_ids[i:i+max_length]\n",
    "            target_chunk=token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6eeeffb-3ee8-49a5-ac4f-ae1f1cac023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(text, batch_size=4, max_length=256, stride=128, shuffle=True,\n",
    "                         drop_last=True, num_workers=0):\n",
    "    #initaalize the tokenizer\n",
    "    tokenizer= tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    #Create dataSet\n",
    "    dataset= GPTDatasetV1(text,tokenizer,max_length,stride)\n",
    "\n",
    "    #Create dataloader\n",
    "    dataloader= DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "967685a2-b460-48cc-9b19-19442fd5517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 50257\n",
    "output_dim= 256\n",
    "\n",
    "token_embedding_layer= torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d29ff39f-765f-4c8d-aaeb-90a2b1171f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data loader\n",
    "# Batch size in data loader will be 8 => means 8 rows in each batch\n",
    "# each row will contain 4 input tokens as context size is 4 to predict next word\n",
    "#Dimenssion will be 256\n",
    "# stride will be 4=> means gap between chunks is 4\n",
    "# 8x4x256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e9ac6a9-0c5a-4300-8da2-3efb31570c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_Length= 4\n",
    "dataloader= create_dataloader_v1(\n",
    "    raw_text, batch_size=8,max_length=max_Length,\n",
    "    stride=max_Length,shuffle=False\n",
    ")\n",
    "data_iter= iter(dataloader)\n",
    "inputs, targets= next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "696bea8a-1e9e-4816-ae21-4b06d5f12ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Ids: \n",
      "  tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token Ids: \\n \", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93762b89-8bf5-4269-a719-fb86c559a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each token in this input, one embedding of vector 256 length is generated\n",
    "# like for 40, there will be a vector of 256 length of embedding vector, for 367.. 1464\n",
    "#each has a 256-length of dimensional vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68e7edd5-36eb-4225-84b7-2be5c045e9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "toke_embeddings= token_embedding_layer(inputs)\n",
    "print(toke_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778f3f2-c946-4754-b7bf-4445452cf4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add positional embedding to each token embedding\n",
    "# there will be only 4 positions as out context size is the batch is 4\n",
    "# but there will be 256 dimensions as per dimensional size it will remain same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e6a507f-a954-457d-a1d1-9e1040b0d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length= max_Length\n",
    "pos_embedding_layer= torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8961ea18-cc44-4083-a406-551d3545aaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings=  pos_embedding_layer(torch.arange(max_Length))\n",
    "print(pos_embeddings.shape)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d03f2303-81a9-4fbf-8212-2d2612f58ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# ass pos embedding with token e\n",
    "input_embeddings= toke_embeddings+ pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
