{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "671974be-9595-47c2-b2b0-65a3c384861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/the-verdict.txt\",\"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "096a04e3-a410-4226-99aa-1795f5fbf694",
   "metadata": {},
   "source": [
    "Word2Vec\n",
    "Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.\n",
    "The phrases were obtained using a simple data-driven approach described in\n",
    "'Distributed Representations of Words and Phrases and their Compositionality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e6b8c1-c9ea-4458-9f4d-b77a6c41b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75be0c07-ac95-4b09-b533-bcb10fc0f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04000ad5-4f8a-4790-96b5-20ad0ef8d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682a4fb2-4e7c-4f27-8486-9bf96c718335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
      " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
      "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
      "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
      " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
      " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
      "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
      "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
      " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
      " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
      " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
      "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
      " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
      "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
      "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
      "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
      " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
      " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
      "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
      "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
      "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
      "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
      " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
      " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
      "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
      "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
      " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
      "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
      " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
      " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
      " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
      " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
      "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
      " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
      "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
      "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
      " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
      " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
      " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
      " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
      " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
      " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
      " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
      " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
      "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
      " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
      "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
      "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
      " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
      "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
      " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
      "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
      " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
      " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
      " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
      " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
      " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
      " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
      " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
      "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
      "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
      " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
      "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
      " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
      "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
      " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
      "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
      " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
      " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
      " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
      "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
      "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
      "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
      "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
      "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors['computer'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7913cdda-7458-4c24-a0dd-c6dab14273c2",
   "metadata": {},
   "source": [
    "print(word_vectors['cat'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a858536-4861-43ef-a3a2-9ae75d740d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#King + Women-Man=?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b451c5-3708-46ef-aeb8-ecdc885256e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.4827326238155365), ('queens', 0.466781347990036), ('kumaris', 0.4653734564781189), ('kings', 0.4558638632297516), ('womens', 0.422832190990448), ('princes', 0.4176960587501526), ('Al_Anqari', 0.41725507378578186), ('concubines', 0.4011078476905823), ('monarch', 0.39624831080436707), ('monarchy', 0.39430153369903564)]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.most_similar(positive=['king','women'],negative=['man'],topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7aa2bb7-53ec-4417-805e-ae5fb703c6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76640123\n",
      "0.6510957\n",
      "0.76434743\n",
      "0.8543272\n",
      "0.7594367\n",
      "0.11408083\n"
     ]
    }
   ],
   "source": [
    "# Example of calculating similarity\n",
    "print (word_vectors.similarity ('woman', 'man'))\n",
    "print (word_vectors.similarity('king', 'queen'))\n",
    "print(word_vectors.similarity ('uncle', 'aunt'))\n",
    "print(word_vectors.similarity('boy', 'girl'))\n",
    "print (word_vectors.similarity('nephew', 'niece'))\n",
    "print (word_vectors.similarity( 'paper', 'water'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ebcd7-31fe-4b0a-a6c0-b2d9e69f6f56",
   "metadata": {},
   "source": [
    "## Most similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abae6046-70a9-4218-98ca-c8dfef639281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('towers', 0.8531749248504639), ('skyscraper', 0.6417425870895386), ('Tower', 0.639177143573761), ('spire', 0.5946877598762512), ('responded_Understood_Atlasjet', 0.5931612849235535)]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.most_similar(\"tower\",topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fa95e-cbf3-45ee-8f1f-1370f857f8bd",
   "metadata": {},
   "source": [
    "## Lets See Vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cfc8230-9342-48f8-8227-91c4f422d324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The magnitude of the difference between 'man' and 'woman' is 2.66\n",
      "The magnitude of the difference between 'semiconductor' and 'earthworm' is 4.27\n",
      "The magnitude of the difference between 'newhew' and 'niece' is 2.87\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#words to compare\n",
    "word1= 'man'\n",
    "word2='woman'\n",
    "\n",
    "word3= 'semiconductor'\n",
    "word4='earthworm'\n",
    "\n",
    "word5= 'newhew'\n",
    "word6='niece'\n",
    "\n",
    "#calculate the vector difference\n",
    " \n",
    "vector_difference1=model[word1]=model[word2]\n",
    "vector_difference2=model[word3]=model[word4]\n",
    "vector_difference3=model[word5]=model[word6]\n",
    "\n",
    "#Calcualte the magnitude of the vector difference\n",
    "magnitude_of_difference1 = np.linalg.norm(vector_difference1)\n",
    "magnitude_of_difference2 = np.linalg.norm(vector_difference2)\n",
    "magnitude_of_difference3 = np.linalg.norm(vector_difference3)\n",
    "\n",
    "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word1,word2,magnitude_of_difference1))\n",
    "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word3,word4,magnitude_of_difference2))\n",
    "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word5,word6,magnitude_of_difference3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05991735-c682-4c5a-a7de-d09a0c8ed61a",
   "metadata": {},
   "source": [
    "we need to add weights/ vector dimensions in for vector embedding,\n",
    "in the start we will use random weights and then based on the token ids we\n",
    "will train and optimise it during training\n",
    "vocabulary size is = Vector_Ids for GPT2 50257x768 (Vector_Ids* weights).\n",
    "During the training of GPT we feed vector embeding and then train, during training \n",
    "We train next word prediction as well as itself embed ding to optimise weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166d2ae-a0e8-49b9-9a52-99f41a54bd0b",
   "metadata": {},
   "source": [
    "## Creating token embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee3a2d-2753-4ec1-9b03-99bcaa8887e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's say we have 4 token ids\n",
    "# suppose vecotrs 'quick', 'fox', 'is', 'in', 'the', 'house'\n",
    "#we will arrange them in assending order and assing token ids\n",
    "#'quick'->[4], 'fox'->[0], 'is'->[3], 'in'->[2], 'the'->[5], 'house'->[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4f3394d-2886-446f-9e9b-54084628657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4178) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/gensim_env/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/gensim_env/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/gensim_env/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m  \u001b[33m0:00:48\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [torch]32m5/6\u001b[0m [torch]]x]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.20.0 fsspec-2025.9.0 mpmath-1.3.0 networkx-3.5 sympy-1.14.0 torch-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c435f0d-9afc-4250-a57e-4c84e528556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cdc9543-22c2-4359-932b-9b706bacda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids= torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "372f0d1f-bfe8-4376-9903-76e7837fc1b0",
   "metadata": {},
   "source": [
    "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
    "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch, setting the random seed to 123 for reproducibility purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2fc961d-9c89-4cb5-84dc-a89932c06d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will take vocabulary size 6 as we have 6 words\n",
    "# and vector dimensions / weights =3\n",
    "\n",
    "vocab_size=6\n",
    "output_dim= 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer= torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb25ab9-a02f-4573-b689-c0021f3cdb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#The print statement in the code prints the embedding layer's underlying weight matrix:\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "729ce8d5-28ae-4068-bc0b-add786e1c8d9",
   "metadata": {},
   "source": [
    "We can see that the weight matrix of the embedding layer contains small, random values. These values are optimized during LLM training as part of the LLM optimization itself, as we will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows and three columns. There is one row for ead the six possible tokens in the vocabulary. And there is one column for each of the three embedding dimensions.\n",
    "\n",
    "#After we instantiated the embedding layer, let's now apply it to a token ID to obtain the embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9637e8-965f-4849-bb63-fb110973778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    " print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d72cbac-9bff-45dd-a3f9-26c633228ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we compare the embedding vector for token ID 3 to the previous embedding matrix, \n",
    "#we see that it is identical to the 4th row\n",
    "\n",
    "\n",
    "# What is an embedding layer?\n",
    "#the embedding layer is essentially a look-up operation that retrieves\n",
    "#rows from the embedding layer's weight matrix via a token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9632710-0d5d-4960-b6a5-f14d9339091e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbbe9c1-7523-4792-8777-656ac2746696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ba94d-8073-4549-b04b-2cc5fe0eab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both embedding layer and Neural Network linear layer lead to same output\n",
    "# Embedding layer is much more computationally efficient, since NN linear layer has many\n",
    "# unnecessary multiplications with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d149c0c-8107-4c9e-9402-4581d36160bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef1e60-a80a-4454-81b6-def4b6c295ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07938ef3-0e21-49bc-8413-0de4d39fee8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gensim Env",
   "language": "python",
   "name": "gensim_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
